{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "1dcb60f3",
   "metadata": {},
   "source": [
    "# Shallow Neural Network: NumPy vs TensorFlow\n",
    "\n",
    "This notebook compares a **from-scratch NumPy implementation** and a **TensorFlow implementation** of a shallow neural network with backpropagation."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "299e4154",
   "metadata": {},
   "source": [
    "## NumPy Implementation "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "9bcfe7ac",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 0, Loss: 0.6933\n",
      "Epoch 200, Loss: 0.6887\n",
      "Epoch 400, Loss: 0.6240\n",
      "Epoch 600, Loss: 0.5681\n",
      "Epoch 800, Loss: 0.5146\n"
     ]
    }
   ],
   "source": [
    "\n",
    "import numpy as np\n",
    "\n",
    "X = np.array([[1,2],[2,3],[3,4],[4,5],[5,6],[6,7]], dtype=float)\n",
    "y = np.array([[0],[0],[0],[1],[1],[1]], dtype=float)\n",
    "\n",
    "m, n = X.shape\n",
    "np.random.seed(42)\n",
    "\n",
    "W1 = np.random.randn(n, 4) * 0.01\n",
    "b1 = np.zeros((1, 4))\n",
    "W2 = np.random.randn(4, 1) * 0.01\n",
    "b2 = np.zeros((1, 1))\n",
    "\n",
    "def relu(Z):\n",
    "    return np.maximum(0, Z)\n",
    "\n",
    "def relu_derivative(Z):\n",
    "    return (Z > 0).astype(float)\n",
    "\n",
    "def sigmoid(Z):\n",
    "    return 1 / (1 + np.exp(-Z))\n",
    "\n",
    "lr = 0.01\n",
    "epochs = 1000\n",
    "\n",
    "for epoch in range(epochs):\n",
    "    Z1 = X @ W1 + b1\n",
    "    A1 = relu(Z1)\n",
    "    Z2 = A1 @ W2 + b2\n",
    "    A2 = sigmoid(Z2)\n",
    "\n",
    "    loss = -np.mean(y*np.log(A2+1e-8)+(1-y)*np.log(1-A2+1e-8))\n",
    "\n",
    "    dZ2 = A2 - y\n",
    "    dW2 = A1.T @ dZ2 / m\n",
    "    db2 = np.mean(dZ2, axis=0, keepdims=True)\n",
    "\n",
    "    dA1 = dZ2 @ W2.T\n",
    "    dZ1 = dA1 * relu_derivative(Z1)\n",
    "    dW1 = X.T @ dZ1 / m\n",
    "    db1 = np.mean(dZ1, axis=0, keepdims=True)\n",
    "\n",
    "    W1 -= lr * dW1\n",
    "    b1 -= lr * db1\n",
    "    W2 -= lr * dW2\n",
    "    b2 -= lr * db2\n",
    "\n",
    "    if epoch % 200 == 0:\n",
    "        print(f\"Epoch {epoch}, Loss: {loss:.4f}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "37fd6062",
   "metadata": {},
   "source": [
    "## TensorFlow Implementation "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "50b1b4ed",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\user\\anaconda3\\Lib\\site-packages\\keras\\src\\export\\tf2onnx_lib.py:8: FutureWarning: In the future `np.object` will be defined as the corresponding NumPy scalar.\n",
      "  if not hasattr(np, \"object\"):\n",
      "C:\\Users\\user\\anaconda3\\Lib\\site-packages\\keras\\src\\layers\\core\\dense.py:106: UserWarning: Do not pass an `input_shape`/`input_dim` argument to a layer. When using Sequential models, prefer using an `Input(shape)` object as the first layer in the model instead.\n",
      "  super().__init__(activity_regularizer=activity_regularizer, **kwargs)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 0, Loss: 0.9658\n",
      "Epoch 200, Loss: 0.3170\n",
      "Epoch 400, Loss: 0.1258\n",
      "Epoch 600, Loss: 0.0662\n",
      "Epoch 800, Loss: 0.0365\n"
     ]
    }
   ],
   "source": [
    "\n",
    "import tensorflow as tf\n",
    "\n",
    "X_tf = tf.constant(X, dtype=tf.float32)\n",
    "y_tf = tf.constant(y, dtype=tf.float32)\n",
    "\n",
    "model = tf.keras.Sequential([\n",
    "    tf.keras.layers.Dense(4, activation='relu', input_shape=(2,)),\n",
    "    tf.keras.layers.Dense(1, activation='sigmoid')\n",
    "])\n",
    "\n",
    "loss_fn = tf.keras.losses.BinaryCrossentropy()\n",
    "optimizer = tf.keras.optimizers.Adam(learning_rate=0.01)\n",
    "\n",
    "for epoch in range(1000):\n",
    "    with tf.GradientTape() as tape:\n",
    "        y_pred = model(X_tf, training=True)\n",
    "        loss = loss_fn(y_tf, y_pred)\n",
    "\n",
    "    grads = tape.gradient(loss, model.trainable_variables)\n",
    "    optimizer.apply_gradients(zip(grads, model.trainable_variables))\n",
    "\n",
    "    if epoch % 200 == 0:\n",
    "        print(f\"Epoch {epoch}, Loss: {loss.numpy():.4f}\")\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python [conda env:base] *",
   "language": "python",
   "name": "conda-base-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
